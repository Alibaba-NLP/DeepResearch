# DeepResearch Environment Configuration
# Copy this file to .env and fill in your actual values
# Run: cp .env.example .env

# =============================================================================
# TORCH/NCCL Configuration (for multi-GPU setups)
# =============================================================================
TORCHDYNAMO_VERBOSE=1
TORCHDYNAMO_DISABLE=1
NCCL_IB_TC=16
NCCL_IB_SL=5
NCCL_IB_GID_INDEX=3
NCCL_SOCKET_IFNAME=eth
NCCL_DEBUG=INFO
NCCL_IB_HCA=mlx5
NCCL_IB_TIMEOUT=22
NCCL_IB_QPS_PER_CONNECTION=8
NCCL_MIN_NCHANNELS=4
NCCL_NET_PLUGIN=none
GLOO_SOCKET_IFNAME=eth0

# =============================================================================
# DeepResearch Configuration
# =============================================================================
QWEN_DOC_PARSER_USE_IDP=false
QWEN_IDP_ENABLE_CSI=false
NLP_WEB_SEARCH_ONLY_CACHE=false
NLP_WEB_SEARCH_ENABLE_READPAGE=false
NLP_WEB_SEARCH_ENABLE_SFILTER=false
QWEN_SEARCH_ENABLE_CSI=false
SPECIAL_CODE_MODE=false
PYTHONDONTWRITEBYTECODE=1

# =============================================================================
# Model and Inference Hyperparameters
# =============================================================================
MODEL_PATH=/your/model/path
DATASET=your_dataset_name
OUTPUT_PATH=/your/output/path
ROLLOUT_COUNT=3
TEMPERATURE=0.85
PRESENCE_PENALTY=1.1
MAX_WORKERS=30

# =============================================================================
# API Keys and External Services
# =============================================================================

# Web Search Providers (in order of quality/preference)
# The system will try each provider in order until one succeeds.
# You only need ONE provider configured, but having multiple provides fallback.

# Exa.ai - Best semantic/neural search ($10 free credits)
# Get your key from: https://exa.ai/
EXA_API_KEY=your_key

# Tavily - Purpose-built for RAG/LLMs (1,000 free requests/month)
# Get your key from: https://tavily.com/
TAVILY_API_KEY=your_key

# Serper API for Google search results (2,500 free queries)
# Get your key from: https://serper.dev/
SERPER_KEY_ID=your_key

# DuckDuckGo is always available as final fallback (FREE, no API key needed)

# Jina API for web page reading
# Get your key from: https://jina.ai/
JINA_API_KEYS=your_key

# Summary model API (OpenAI-compatible) for page summarization
# Get your key from: https://platform.openai.com/
API_KEY=your_key
API_BASE=your_api_base
SUMMARY_MODEL_NAME=your_summary_model_name

# Dashscope API for file parsing (PDF, Office, etc.)
# Get your key from: https://dashscope.aliyun.com/
# Supports: qwen-omni-turbo, qwen-plus-latest
DASHSCOPE_API_KEY=your_key
DASHSCOPE_API_BASE=your_api_base
VIDEO_MODEL_NAME=your_video_model_name
VIDEO_ANALYSIS_MODEL_NAME=your_analysis_model_name

# =============================================================================
# Python Code Execution Sandbox
# =============================================================================

# SandboxFusion endpoints for Python interpreter
# Example: "http://22.16.67.220:8080,http://22.16.78.153:8080,http://22.17.10.216:8080"
# See: https://github.com/bytedance/SandboxFusion
SANDBOX_FUSION_ENDPOINT=your_sandbox_endpoint
TORCH_COMPILE_CACHE_DIR=./cache

# =============================================================================
# IDP Service (Advanced File Parsing - Optional)
# =============================================================================

# IDP service for enhanced file parsing
# Set USE_IDP=True and provide credentials for more powerful parsing
# Documentation: https://help.aliyun.com/zh/document-mind/developer-reference/use-idp-llm-to-complete-document-summary
USE_IDP=False
IDP_KEY_ID=your_idp_key_id
IDP_KEY_SECRET=your_idp_key_secret

# =============================================================================
# Multi-Worker Configuration (Optional)
# =============================================================================

# These are typically set by distributed training frameworks
# WORLD_SIZE=1
# RANK=0

# =============================================================================
# llama.cpp Local Inference (Alternative for Mac/Local Users)
# =============================================================================
# If using the llama.cpp local inference option instead of vLLM:

# The llama.cpp server URL (default works if using start_llama_server.sh)
LLAMA_SERVER_URL=http://127.0.0.1:8080

# For llama.cpp mode:
# - Web search uses DuckDuckGo by default (FREE, no API key needed)
# - JINA_API_KEYS is optional but recommended for better page reading
# - See: python inference/interactive_llamacpp.py --help